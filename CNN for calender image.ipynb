{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "ll '/content/drive/My Drive/mafs_5440_group2/img_data/monthly_20d/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY6933sOT4Rg",
        "outputId": "fb0e1869-8aa2-4eb7-afbd-2d753c4e1c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/My Drive/mafs_5440_group2/img_data/monthly_20d/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VAWwHp6s3N6",
        "outputId": "05b779f0-c3f9-4762-d0ee-c1bc923ed877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "al6eTGjvSylt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "\n",
        "import torch\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torchsummary import summary\n",
        "import re\n",
        "import time\n",
        "import scipy.stats\n",
        "import torch.optim as optim\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n"
      ],
      "metadata": {
        "id": "Be0Dtvi0hLHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1=nn.Conv2d(in_channels=1,out_channels=64,kernel_size=(5,3),padding=(7,1),stride=(3,1))\n",
        "        self.batchnorm1=nn.BatchNorm2d(64,affine=True)\n",
        "        self.relu1=nn.LeakyReLU()\n",
        "        self.pool1=nn.MaxPool2d(kernel_size=(2,1))\n",
        "        self.conv2=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=(5,3),padding=(2,1),stride=1)\n",
        "        self.batchnorm2=nn.BatchNorm2d(128,affine=True)\n",
        "        self.relu2=nn.LeakyReLU()\n",
        "        self.pool2=nn.MaxPool2d(kernel_size=(2,1))\n",
        "        self.conv3=nn.Conv2d(in_channels=128,out_channels=256,kernel_size=(5,3),padding=(2,1),stride=1)\n",
        "        self.batchnorm3=nn.BatchNorm2d(256,affine=True)\n",
        "        self.relu3=nn.LeakyReLU()\n",
        "        self.pool3=nn.MaxPool2d(kernel_size=(2,1))\n",
        "        self.fc= nn.Linear(46080,2)\n",
        "    def forward(self,x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.batchnorm1(y)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.batchnorm2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = self.conv3(y)\n",
        "        y = self.batchnorm3(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.pool3(y)\n",
        "        y = y.view(y.shape[0],-1)\n",
        "        y = self.fc(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "def data_set(dir, year_index, in_size, out_size,height,width):\n",
        "    year = year_index[0]\n",
        "    img_path = os.path.join(dir,\"monthly_20d\",f\"20d_month_has_vb_[20]_ma_{year}_images.dat\")\n",
        "    label_path = os.path.join(dir,\"monthly_20d\",f\"20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather\")\n",
        "    image = np.memmap(img_path,dtype=np.uint8,mode='r').reshape(-1,height[in_size],width[out_size])\n",
        "    label = pd.read_feather(label_path)\n",
        "\n",
        "    for year in year_index[1:]:\n",
        "        img_path = os.path.join(dir,\"monthly_20d\",f\"20d_month_has_vb_[20]_ma_{year}_images.dat\")\n",
        "        label_path = os.path.join(dir,\"monthly_20d\",f\"20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather\")\n",
        "        img = np.memmap(img_path,dtype=np.uint8,mode='r').reshape(-1,height[in_size],width[out_size])\n",
        "        lbl = pd.read_feather(label_path)\n",
        "        image = np.concatenate((image,img),axis=0)\n",
        "        label = pd.concat([label,lbl])\n",
        "    return image, label\n",
        "\n",
        "def sampling(image, label):\n",
        "    len_df = len(label)\n",
        "    random_index = np.random.permutation(len_df)\n",
        "    train_index = random_index[:int(0.7*len_df)]\n",
        "    train_image = image[train_index]\n",
        "    train_label = label.iloc[train_index,:]\n",
        "    validation_index = random_index[int(0.7*len_df):]\n",
        "    validation_image = image[validation_index]\n",
        "    validation_label = label.iloc[validation_index,:]\n",
        "    return train_image, train_label, validation_image,validation_label\n",
        "\n",
        "\n",
        "class Stock(torch.utils.data.Dataset):\n",
        "    def __init__(self, image, label,transform):\n",
        "        self.image = image\n",
        "        self.label = label\n",
        "        self.transform = transform\n",
        "        self.pre_process()\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "    def __getitem__(self, index):\n",
        "        image = self.transform(self.image[index])\n",
        "        return image, self.label.iloc[index, -1]\n",
        "    def pre_process(self):\n",
        "        self.label['Label_20d'] = self.label['Ret_20d']\n",
        "        self.label.loc[self.label['Label_20d']<=0, 'Label_20d']=0\n",
        "        self.label.loc[self.label['Label_20d']>0, 'Label_20d']=1\n",
        "        self.label.loc[pd.isnull(self.label['Label_20d']), 'Label_20d']=0\n",
        "\n",
        "\n",
        "def create_loader(img_dir,batch_size):\n",
        "  IMAGE_WIDTH={5:15, 20:60, 60:180}\n",
        "  IMAGE_HEIGHT={5:32, 20:64, 60:96}\n",
        "\n",
        "  in_size = 20\n",
        "  out_size = 20\n",
        "\n",
        "  train_index = np.arange(1993, 2007, 1)\n",
        "  image, label = data_set(img_dir, train_index, in_size, out_size,IMAGE_HEIGHT,IMAGE_WIDTH)\n",
        "  train_image, train_label, validation_image,validation_label = sampling(image, label)\n",
        "\n",
        "\n",
        "  test_index = np.arange(2007, 2020, 1)\n",
        "  test_image, test_label = data_set(img_dir, test_index, in_size, out_size,IMAGE_HEIGHT,IMAGE_WIDTH)\n",
        "\n",
        "  mytransform = torchvision.transforms.ToTensor()\n",
        "  train_data = Stock(train_image, train_label, mytransform)\n",
        "  validation_data = Stock(validation_image, validation_label, mytransform)\n",
        "  test_data = Stock(test_image, test_label, mytransform)\n",
        "\n",
        "  train_loader=torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "  validation_loader=torch.utils.data.DataLoader(dataset=validation_data, batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "  test_loader=torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True,num_workers=4)\n",
        "  loaders = {'train': train_loader, 'validation': validation_loader, 'test': test_loader}\n",
        "\n",
        "  return loaders\n",
        "\n",
        "class AverageMeter():\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def accuracy(output, target):\n",
        "    batch_size = target.size(0)\n",
        "    _, pred = output.topk(1, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    res.append(correct.sum()/batch_size*100)\n",
        "    return res\n",
        "\n",
        "def train_model(model, data_loader, criterion, optimizer, scheduler = None):\n",
        "    epoch = 1\n",
        "    stopping = 0\n",
        "    iter = 0\n",
        "    log_saver = {\n",
        "    'train_loss': [],\n",
        "    'train_error': [],\n",
        "    'validation_loss': [],\n",
        "    'validation_error': [],\n",
        "    'test_loss':[],\n",
        "    'test_error':[]\n",
        "    }\n",
        "    while(stopping < 2):\n",
        "        iter += 1\n",
        "        print('Epoch {}'.format(epoch))\n",
        "        print('-' * 10)\n",
        "        for mode in ['train', 'validation']:\n",
        "\n",
        "            loss_meter = AverageMeter()\n",
        "            acc_meter = AverageMeter()\n",
        "\n",
        "            if mode == 'train':\n",
        "                model.train(True)\n",
        "            else:\n",
        "                model.train(False)\n",
        "\n",
        "            for i, data in enumerate(data_loader[mode]):\n",
        "                inputs, labels = data\n",
        "                if use_gpu:\n",
        "                    inputs = inputs.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs.data, 1)\n",
        "                # print(labels)\n",
        "\n",
        "                # print(labels)\n",
        "                labels = labels.long()\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if mode == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    if scheduler is not None:\n",
        "                      scheduler.step()\n",
        "\n",
        "\n",
        "                loss_meter.update(loss.data.item(), outputs.shape[0])\n",
        "                acc_meter.update(\n",
        "                    accuracy(outputs.data, labels.data)[-1].item(), outputs.shape[0])\n",
        "\n",
        "            epoch_loss = loss_meter.avg\n",
        "            epoch_error = 1 - acc_meter.avg / 100\n",
        "\n",
        "            if mode == 'train':\n",
        "                log_saver['train_loss'].append(epoch_loss)\n",
        "                log_saver['train_error'].append(epoch_error)\n",
        "\n",
        "            elif mode == 'validation':\n",
        "\n",
        "                log_saver['validation_loss'].append(epoch_loss)\n",
        "                log_saver['validation_error'].append(epoch_error)\n",
        "\n",
        "                if iter > 2 and (log_saver['validation_loss'][-1] >= log_saver['validation_loss'][-2]):\n",
        "                    stopping += 1\n",
        "                else:\n",
        "                    stopping = 0\n",
        "\n",
        "\n",
        "            print(\n",
        "                f'{mode} loss: {epoch_loss:.4f}; error: {epoch_error:.4f}'\n",
        "            )\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Testing...\")\n",
        "    loss_meter = AverageMeter()\n",
        "    acc_meter = AverageMeter()\n",
        "\n",
        "\n",
        "    model.train(False)\n",
        "\n",
        "    for i, data in enumerate(data_loader[\"test\"]):\n",
        "        inputs, labels = data\n",
        "        if use_gpu:\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        labels = labels.long()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_meter.update(loss.data.item(), outputs.shape[0])\n",
        "        acc_meter.update(\n",
        "            accuracy(outputs.data, labels.data)[-1].item(), outputs.shape[0])\n",
        "\n",
        "    epoch_loss = loss_meter.avg\n",
        "    epoch_error = 1 - acc_meter.avg / 100\n",
        "    log_saver['test_loss'].append(epoch_loss)\n",
        "    log_saver['test_error'].append(epoch_error)\n",
        "\n",
        "    print(f'test loss: {epoch_loss:.4f}; error: {epoch_error:.4f}')\n",
        "\n",
        "    return model, log_saver"
      ],
      "metadata": {
        "id": "hU68wEr9tFs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = \"/content/drive/My Drive/mafs_5440_group2/img_data/\"\n",
        "batch_size = 128\n",
        "data_loader = create_loader(img_dir,batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HH9lXMt9zfK",
        "outputId": "0bfbd8da-1c7f-422b-aea3-e9525bf64803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-de1cf05bcfd4>:76: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.label['Label_20d'] = self.label['Ret_20d']\n",
            "<ipython-input-33-de1cf05bcfd4>:76: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.label['Label_20d'] = self.label['Ret_20d']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model=Model()\n",
        "\n",
        "# if use_gpu:\n",
        "#   model.cuda()\n",
        "\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "# model, log = train_model(model=model, data_loader=data_loader, criterion=criterion, optimizer=optimizer)\n",
        "\n",
        "# train_validation_loss_error = {key: log[key] for key in ['train_loss', 'train_error', 'validation_loss', 'validation_error']}\n",
        "# train_validation_loss_error = pd.DataFrame(train_validation_loss_error)\n",
        "# train_validation_loss_error.to_csv('/content/drive/My Drive/train_validation_loss_error.csv')\n",
        "\n",
        "# test_loss_error = {key: log[key] for key in ['test_loss', 'test_error']}\n",
        "# test_loss_error = pd.DataFrame(test_loss_error)\n",
        "# test_loss_error.to_csv('/content/drive/My Drive/test_loss_error.csv')"
      ],
      "metadata": {
        "id": "pdapn0JW8BRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter tuning\n",
        "\n",
        "class Model_new(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model_new, self).__init__()\n",
        "        self.conv1=nn.Conv2d(in_channels=1,out_channels=64,kernel_size=(5,3),padding=(7,1),stride=(3,1))\n",
        "        self.batchnorm1=nn.BatchNorm2d(64,affine=True)\n",
        "        self.relu1=nn.LeakyReLU()\n",
        "        self.pool1=nn.MaxPool2d(kernel_size=(2,1))\n",
        "        self.conv2=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=(5,3),padding=(2,1),stride=1)\n",
        "        self.batchnorm2=nn.BatchNorm2d(128,affine=True)\n",
        "        self.relu2=nn.LeakyReLU()\n",
        "        self.pool2=nn.MaxPool2d(kernel_size=(2,1))\n",
        "        self.conv3=nn.Conv2d(in_channels=128,out_channels=256,kernel_size=(5,3),padding=(2,1),stride=1)\n",
        "        self.batchnorm3=nn.BatchNorm2d(256,affine=True)\n",
        "        self.relu3=nn.LeakyReLU()\n",
        "        self.pool3=nn.MaxPool2d(kernel_size=(2,1))\n",
        "        self.fc= nn.Linear(46080,2)\n",
        "    def forward(self,x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.batchnorm1(y)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.batchnorm2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = self.conv3(y)\n",
        "        y = self.batchnorm3(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.pool3(y)\n",
        "        y = y.view(y.shape[0],-1)\n",
        "        y = self.fc(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "9FwIMGsl85mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter tuning\n",
        "\n",
        "model_new=Model_new()\n",
        "\n",
        "if use_gpu:\n",
        "  model_new.cuda()\n",
        "\n",
        "criterion_new = nn.CrossEntropyLoss()\n",
        "optimizer_new = optim.Adam(model_new.parameters(), lr=0.00005)\n",
        "# gamma = 0.9  # exponential decay\n",
        "# scheduler = optim.lr_scheduler.ExponentialLR(optimizer_new, gamma=gamma)\n",
        "\n",
        "\n",
        "model, log = train_model(\n",
        "    model=model_new, data_loader=data_loader, criterion=criterion_new, optimizer=optimizer_new)\n",
        "\n",
        "train_validation_loss_error = {key: log[key] for key in ['train_loss', 'train_error', 'validation_loss', 'validation_error']}\n",
        "train_validation_loss_error = pd.DataFrame(train_validation_loss_error)\n",
        "train_validation_loss_error.to_csv('/content/drive/My Drive/train_validation_loss_error(LR:5e-5).csv')\n",
        "\n",
        "test_loss_error = {key: log[key] for key in ['test_loss', 'test_error']}\n",
        "test_loss_error = pd.DataFrame(test_loss_error)\n",
        "test_loss_error.to_csv('/content/drive/My Drive/test_loss_error(LR:5e-5).csv')"
      ],
      "metadata": {
        "id": "nte0gNjDVOUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d0d1d1-4c6f-432e-ee3d-65d99296a27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "----------\n",
            "train loss: 0.7083; error: 0.4807\n",
            "validation loss: 0.7301; error: 0.4883\n",
            "Epoch 2\n",
            "----------\n",
            "train loss: 0.6960; error: 0.4671\n",
            "validation loss: 0.6890; error: 0.4610\n",
            "Epoch 3\n",
            "----------\n",
            "train loss: 0.6916; error: 0.4578\n",
            "validation loss: 0.6874; error: 0.4559\n",
            "Epoch 4\n",
            "----------\n",
            "train loss: 0.6872; error: 0.4489\n",
            "validation loss: 0.6897; error: 0.4588\n",
            "Epoch 5\n",
            "----------\n",
            "train loss: 0.6808; error: 0.4353\n",
            "validation loss: 0.6901; error: 0.4591\n",
            "Testing...\n",
            "test loss: 0.6990; error: 0.4770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYhoUXkGt_Rc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}